{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of crosculap |  Analyse d'opinion avec un réseau convolutif","provenance":[{"file_id":"13btMcYd8dvsIn7zTLDSFwfkZU1IJ7nm7","timestamp":1615368161341},{"file_id":"1g5Q4wzseOP3-P___KgMZdjg0skRSvCBw","timestamp":1615363580289},{"file_id":"17-92pN5XWwOpYwjToL77mBgJ1QDopagn","timestamp":1615293036616},{"file_id":"1Kp5hoS5cdyIXsW2yO17fLpZm1lPqTORC","timestamp":1615290941505}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"c4lhKPwxHqIM"},"source":["# Analyse d'opinion avec un réseau convolutif\n","\n","En machine learning \"classique\" (c'est-à-dire ce qu'on a fait pour la détection de langue), il y a deux enjeux pour obtenir un score élevé :\n","\n","* trouver une représentation des données utilisables par un classifieur\n","* trouver le classifieur qui saura le mieux en tirer partie\n","\n","On passe donc un certain temps à chercher une représentation des données exploitable. C'est ce qu'on a fait dans le dernier TP, avec `CountVectorizer` puis `TfidfVectorizer`, en affinant la fonction de preprocessing, en cherchant la meilleure configuration de n-grams, etc.\n","\n","Une des **promesses du deep learning**, c'est que lors de l'apprentissage, le réseau apprend lui-même à représenter les données de manière utile pour la tâche en cours. Et plus il y a de couches dans le réseau, plus le réseau saura **calculer des représentations complexes**. Par exemple, en vision par ordinateur (computer vision), une couche pourra identifier des lignes, et une autre plus loin des formes plus complexes, etc.\n","\n","Du coup, en deep learning, les enjeux sont différents. On cherche à savoir :\n","\n","* comment aider le réseau à représenter efficacement les données ?\n","* quelle est la meilleure architecture pour résoudre le problème ?\n","\n","Les réponses à ces questions sont complexes et viennent avec la pratique, mais voyons sur un exemple comment on peut traiter ce problème."]},{"cell_type":"markdown","metadata":{"id":"5FOOpQd5HqIR"},"source":["## Environnement\n","\n","Nous allons ici utiliser un GPU, fourni par Colab si demandé gentiment. Vous pouvez aussi utiliser votre propre GPU en local si vous le souhaitez, mais je n'en ai pas alors je passe par Colab. Pour faire comme moi, aller dans \"Exécution -> Modifier le type d'exécution\" et choisir \"GPU\" dans \"Accélerateur matériel\".\n","\n","Il est aussi possible d'[utiliser des TPUs](https://colab.research.google.com/notebooks/tpu.ipynb) avec Google Colab, mais en pratique c'est plus lent pour les \"petits\" modèles que nous allons utiliser ici, parce qu'il y a un coût initital conséquent.\n","\n","Pour voir si un GPU est là, exécutez cette commande :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDQRpGr2HunZ","executionInfo":{"status":"ok","timestamp":1615379902108,"user_tz":-60,"elapsed":657,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"6eebe06c-690c-41bf-b836-7d882065d002"},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Wed Mar 10 12:38:21 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nCOJECqB1y10"},"source":["Nous allons utiliser Keras, fourni dans TensorFlow 2. Importons les librairies que nous allons utiliser, et déclarons une fonction qui affichera l'historique d'entraînement Keras.\n","\n","(Je viens d'apprendre que [TensorBoard était disponible dans Google Colab](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb#scrollTo=lpUO9HqUKP6z), mais n'ai pas eu le temps de mettre à jour ce notbeook. Si vous connaissez TensorBoard, je vous encourage à vous en servir à la place de `plot_history`.)"]},{"cell_type":"code","metadata":{"id":"HrrR4pZy7bte","executionInfo":{"status":"ok","timestamp":1615379906659,"user_tz":-60,"elapsed":2062,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}}},"source":["%matplotlib inline\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","def plot_history(history):\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('Model accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","\n","pd.set_option('display.max_colwidth', 200)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1BDwX1xIHqIt"},"source":["### Jeu de données\n","\n","Nous allons utiliser le [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), un corpus de critiques de films beaucoup utilisé en NLP. On a 25k critiques pour l'ensemble d'entraînement (moitié positives, moitiés négatives), et 25k critiques pour l'ensemble de test (même distribution), qu'on va redécouper en ensembles d'entraîment, validation et test.\n","\n","Récupérons les données depuis le site officiel."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7olRzYijP4AG","executionInfo":{"status":"ok","timestamp":1615379918393,"user_tz":-60,"elapsed":10612,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"b3a3a74e-bb08-4833-ac22-1da9b990e884"},"source":["!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2021-03-10 12:38:28--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘aclImdb_v1.tar.gz’\n","\n","aclImdb_v1.tar.gz   100%[===================>]  80.23M  24.8MB/s    in 3.5s    \n","\n","2021-03-10 12:38:31 (23.2 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E1R0XsVo23mf"},"source":["Puis lisons-les et réservons une partie de l'ensemble d'entraînement pour la validation. Même s'il est étonnamment large, nous ne touchons pas à l'ensemble de test pour pouvoir [nous comparer à la littéreature](https://paperswithcode.com/sota/sentiment-analysis-on-imdb)."]},{"cell_type":"code","metadata":{"id":"rD0tB0_7P7oP","executionInfo":{"status":"ok","timestamp":1615379919669,"user_tz":-60,"elapsed":9454,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}}},"source":["from pathlib import Path\n","from sklearn.model_selection import train_test_split\n","\n","def read_imdb_split(split_dir):\n","    split_dir = Path(split_dir)\n","    texts = []\n","    labels = []\n","    for label_dir in [\"pos\", \"neg\"]:\n","        for text_file in (split_dir / label_dir).iterdir():\n","            texts.append(text_file.read_text())\n","            labels.append(0 if label_dir is \"neg\" else 1)\n","\n","    return texts, labels\n","\n","X_train, y_train = read_imdb_split('aclImdb/train')\n","X_test, y_test = read_imdb_split('aclImdb/test')\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=.2, random_state=0, stratify=y_train)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ETYuD7Dl9ta_"},"source":["Affichons quelques exemples des données d'entraînement tirés au hasard :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p2bItQW7ThjA","executionInfo":{"status":"ok","timestamp":1615379919672,"user_tz":-60,"elapsed":6491,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"ea82abc6-f207-4f93-b9d6-1bbd5b9f3395"},"source":["import random\n","\n","for i in random.choices(range(len(X_train)), k=10):\n","    print(\"positive\" if y_train[i] else \"negative\", X_train[i][:100] + \"…\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["positive This is a generally nice film, with good story, great actors and great songs. The cinematography was…\n","positive The first of two Jim Thompson adaptations released in 1990 (the other being the more well-known GRIF…\n","positive Ealing Studios, a television and film production company based in West London, claims to be the olde…\n","negative The plot sounded like it had promise. To be honest I did not watch the entire movie. After about an …\n","positive I just viewed the film two days ago, and I was filled with anticipation, being that Paris is my seco…\n","negative This is a weak film with a troubled history of cuts and re-naming. It doesn't work at all. Firstly t…\n","negative I thought this was one of the most depressing holiday movies I have ever seen--the others being THE …\n","positive I really think that this movie is great, personally. But, in every movie there is a downer. Now, som…\n","negative LAGE RAHO MUNNABHAI is really a disappointing movie . I have seen the first part of MUNNABHAI and it…\n","positive \"Masks\" is a moving film that works on many levels. At its simplest, it is the haunting story of a s…\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IBwu-7Ry4OVa"},"source":["Il est intéressant de noter que les phrases sont très longues.\n","\n","Un autre point essentiel ici c'est que sur IMDb, les notes sont indiquées en dehors du texte, donc le commentaire n'a pas besoin de dire que c'est bon ou mauvais : il ne fait que donner ses arguments, parfois avec ironie. Autrement dit, on ne va pas trouver de phrases comme \"Ce film est mauvais\", c'est souvent plus subtil que ça.\n","\n","La conséquence de ces deux points (subtilité et textes longs) est que si on donne un texte court qui dit clairement que le film est mauvais, notre modèle a de bonnes chances de se tromper.\n","\n","**Exercice 1**\n","\n","Afficher avec pandas une boîte à moustache (box plot) pour voir la distribution du nombre de mots dans les textes. Pour calculer le nombre de mots, on peut utiliser une approximation horrible et couper sur les espaces : `len(text.split(\" \"))`.\n","\n","<details>\n","<summary>Indices</summary>\n","\n"," * `pd.Series(train_texts)` retourne une `Series` panda\n"," * `apply()` permet d'appliquer une fonction à chaque texte\n"," * [plot.box()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.plot.box.html#pandas.Series.plot.box) affiche la boîte à moustache\n","</details>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"Zhc3X_875Hua","executionInfo":{"status":"ok","timestamp":1615379920206,"user_tz":-60,"elapsed":3142,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"383cacca-4a9e-4a13-e0bc-41f32688c400"},"source":["liste_mot=[]\n","for i in X_train:\n","  liste_mot.append(len(i.split(' ')))\n","\n","df=pd.DataFrame(liste_mot)\n","boxplot= df.boxplot()"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"uPlWZOU8ZPfV"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"HOWy7FEXHqJK"},"source":["**Exercice 2**\n","\n","Sachant qu'il n'y a que deux classes (positif et négatif), quel est le score que peut atteindre un classifieur qui répond au hasard ?"]},{"cell_type":"code","metadata":{"id":"Udx_e9_tOgw9"},"source":["# Sachant qu'il y a que deux classes (positifs et négatifs) le score d'un \n","# classifieur qui répond au hasard est de 50%"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"4Nq6n0A_nAl2","executionInfo":{"status":"ok","timestamp":1615371658281,"user_tz":-60,"elapsed":1465,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"bb080697-2cf7-4b03-e42c-3024b8e232e2"},"source":["X_train[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The trick to creating a good, solid mystery story is as much a matter of timing as its about plot contrivances, colorful characters or surprising twists. Anyone who has ever labored in frustration with an un-finishable Sunday New York Times crossword knows that any puzzle that takes too long to solve ceases to be any fun. The best murder mysteries, be they on film or in print, are slight affairs that get to the point, spell out their clues, line up their suspects and, hopefully, zap us with a few surprises; being complicated without being unduly confusing. And they play fair; on second, third and fourth viewings of the clues and red herrings we should be just as pleased to marvel at how well it all comes together as we were at being surprised in the first place. Indeed, good thrillers should get better on repeated viewings as we anticipate the double and triple crosses.<br /><br />Sidney Lumet\\'s comedy-thriller DEATHTRAP, as derived from Ira Levin\\'s hit Broadway play, is a great example. It moves along at a tidy clip, skillfully juggling its clues, being (almost) totally honest with us (even when it is lying to us) and yet never revealing where it is going (even when it is telling us where it might go). It is less a murder mystery movie in the traditional vane than it is a movie about murder mysteries, derived from a play about playwriting. Rather than going backward -- a murder and then an investigation to explain why everything happened -- DEATHTRAP leads us through the crime(s) step by step, leaving ample room for the unexpected; as the ads advise it is less a \"whodunit\" than a \"who\\'lldoit.\" <br /><br />DEATHTRAP is often compared (unfavorably, oddly enough) to the play and movie versions of SLEUTH, though in reality it has much more in common with SCREAM, the self-mocking essay on teeny-bopper horror flicks. Like that clever film, DEATHTRAP labels itself (a thriller about thrillers), sets it parameters (\"a one-set, five character moneymaker\") and then proceeds to deconstruct its genre by revealing itself as \"the most outlandish and preposterous set of circumstances entertaining enough to persuade an audience to suspend its disbelief.\" <br /><br />DEATHTRAP bravely gives us a mystery with only five major characters, two of which are of minimum importance. Henry Jones as a cagey lawyer is on hand mostly for exposition (and to supply us with his penchant for folksy charm) and Irene Worth is all quirks and comic relief as a psychic-cum-sleuth who acts as the nominal detective. That leaves three main characters to be the killer(s) and/or the victim(s): It is a testament to Michael Caine\\'s abilities that as Sidney Bruhl, a down-on-his-luck author of mystery plays, he creates a character who we intrinsically like and trust, even as we recognize immediately that almost everything he says is a lie. As his adoring, if somewhat ditzy wife, Myra, Dyan Canon flirts with being over the top by giving a roller-coaster ride of a performance with a character that by turns seems to be frail or overbearing, crafty or hysterical, timid or bold and uncompromisingly in love with a less than reciprocating Sidney. The third angle of this unexpected triangle is a fledgling playwright named Clifford Anderson played by Christopher Reeve in such a way that we never quite get a handle on just who his character is: enthusiastic preppie wannabe writer, semi-innocent victim or cunningly charming sociopath. As the various character dance around each other, the cleverly dour script adapted by ace scribe Jay Presson Allen manages to be consistently amusing, even as it builds suspense. And even after the final twist (an improvement over the play\\'s finale), it may not be quite clear just who has manipulated who to do what.<br /><br />Lumet is by no means a master of comedy, so he lets his able cast have free reign to flesh out the characters and they all give sharp, theatrical, yet subtle work, with Reeve being particularly noteworthy. But what Lumet does so well is to work skillfully in tight quarters. As he did brilliantly in 12 ANGRY MEN, he takes a one-set play, and with a minimum of opening up, manages to make what could have been cramped, stagy and stagnant seem endlessly photogenic and spacious. The setting, a country home converted from an old windmill, is relatively small, but as designed by Tony Walton it manages to be both cozy and charming, as well as spooky and treacherous. It is so truly difficult to tell where the studio set and the real country house cross boundaries that to a degree the set becomes a sixth character. And as the scene of the crime, it is a most inviting deathtrap indeed.'"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"YLa5zdDROhdp"},"source":["### Baseline\n","\n","Avant de s'intéresser à un modèle de deep learning, apprenons un classifieur simple qui servira de référence, ou baseline en anglais : une technique plus évoluée devra obtenir des scores plus élevés."]},{"cell_type":"markdown","metadata":{"id":"_xvutCm6HqJS"},"source":["**Exercice 3**\n","\n","En utilisant à chaque fois les paramètres par défaut, entraîner un classifieur `MultinomialNB` en transformant les données via un `TfidfVectorizer`. Afficher le score sur les données de validation et de test."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-bfvqm6HqJU","executionInfo":{"status":"ok","timestamp":1615379936566,"user_tz":-60,"elapsed":8484,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"25415535-0d61-4c62-824a-eb948b6e25f0"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","\n","# À ton tour !\n","cls = MultinomialNB()\n","\n","vectorizer = TfidfVectorizer()\n","\n","Xcount_train = vectorizer.fit_transform(X_train)\n","Xcount_validation = vectorizer.transform(X_val)\n","Xcount_test = vectorizer.transform(X_test)\n","\n","cls.fit(Xcount_train, y_train)\n","\n","val_score = cls.score(Xcount_validation,y_val)\n","test_score = cls.score(Xcount_test,y_test)\n","\n","# We have enough data to display two digits of precision\n","print(f\"Validation score: {val_score:.2%}\")\n","print(f\"Test score: {test_score:.2%}\")\n","\n","#assert test_score > 0.8"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Validation score: 86.20%\n","Test score: 82.96%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IM6Ec0-BHqJa"},"source":["OK, donc le score sur les données de test (83.10%) sera le score à battre."]},{"cell_type":"markdown","metadata":{"id":"tWYn2ctwHqJc"},"source":["## Keras\n","\n","Pour notre premier modèle de deep learning, nous allons utiliser Keras, une API haut niveau plus simple à utiliser que TensorFlow.\n","\n","Une constante quand on fait du deep learning en NLP, c'est l'utilisation de *word embeddings*. L'idée, c'est que chaque mot soit représenté par un vecteur à N dimensions (N allant en général de 50 à 300), chaque valeur étant entre 0 et 1. Par exemple, le mot \"the\" pourra être représenté avec cinquante nombre réels : `[0.2 0.3 0.1 ... 0.1 0.9]`.\n","\n","Lors de l'apprentissage, le réseau de neurones va modifier ces valeurs afin d'améliorer la classification. À la fin de l'apprentissage, les mots qui ont un sens proche pour la tâche de classification obtiendront des embeddings proches. C'est un des secrets de la puissance du deep learning en NLP : une forte capacité à généraliser en traitant de manière similaires des mots qui ont un sens proche.\n","\n","(Des techniques antérieures au deep learning permettent aussi de généraliser, mais étaient un peu moins efficaces. Voir [*Don’t count, predict!* A systematic comparison of context-counting vs. context-predicting semantic vectors](http://www.aclweb.org/anthology/P14-1023) pour les détails.)\n","\n","En tout cas, avant d'entraîner un réseau de neurones, il faut représenter les textes de manière adaptée : c'est l'objet de la prochaine section."]},{"cell_type":"markdown","metadata":{"id":"yiHu1ugVHqJe"},"source":["### Transformation des textes\n","\n","La transformation consiste à :\n","\n","1. découper (tokenizer) en mots et à assigner un entier à chaque mot et\n","2. représenter les phrases à partir de ces mots.\n","\n","**Exercice 4**\n","\n","Utiliser un [Tokenizer Keras](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) pour transformer `X_train`, `X_val` et `X_test` en listes d'entier. (J'utilise `Xi_...` pour integer.)"]},{"cell_type":"code","metadata":{"id":"X2apEyfkHqJm","executionInfo":{"status":"ok","timestamp":1615379951523,"user_tz":-60,"elapsed":13920,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}}},"source":["# only consider the most 5 000 frequents words\n","VOCABULARY_SIZE = 5_000\n","\n","cnn_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCABULARY_SIZE)\n","\n","def tokenize(lang):\n","  cnn_tokenizer.fit_on_texts(lang)\n","  tensor=cnn_tokenizer.texts_to_sequences(lang)\n","  return tensor\n","\n","Xi_train = tokenize(X_train)\n","Xi_val = tokenize(X_val)\n","Xi_test = tokenize(X_test)\n","\n","assert len(Xi_train) == 20_000\n","assert len(Xi_val) == 5_000\n","assert len(Xi_test) == 25_000"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wSHDIjDD4II_"},"source":["Vérifions que même si le tokenizer a vu 80 000 mots, l'identifiant de mot le plus haut dans `Xi_train` correspond à la taille du vocabulaire :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APw4XUA3p2x1","executionInfo":{"status":"ok","timestamp":1615379953035,"user_tz":-60,"elapsed":1479,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"9d4c330c-04bb-40a1-d6ac-2810acc7ae61"},"source":["len(cnn_tokenizer.index_word)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["124252"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"AzBwU3qLphUG","executionInfo":{"status":"ok","timestamp":1615379956198,"user_tz":-60,"elapsed":1222,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}}},"source":["assert max([max(s) for s in Xi_train]) == VOCABULARY_SIZE - 1"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lqwQaRNWHqJr"},"source":["Cettet numérotation nous permet d'observer les mots les plus fréquents du corpus :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOc5mexSHqJ0","executionInfo":{"status":"ok","timestamp":1615379956725,"user_tz":-60,"elapsed":571,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"e316fd66-61c9-4d05-84e7-9a214697c23b"},"source":["for i in range(1, 20+1):\n","    print(f\"{i}: {cnn_tokenizer.index_word[i]}\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["1: the\n","2: and\n","3: a\n","4: of\n","5: to\n","6: is\n","7: br\n","8: in\n","9: it\n","10: i\n","11: this\n","12: that\n","13: was\n","14: as\n","15: for\n","16: with\n","17: movie\n","18: but\n","19: film\n","20: on\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A2KDSSlGHqJ5"},"source":["Ce sont essentiellement des \"stop words\", des mots qui sont les plus fréquents dans tout texte en anglais, et qu'on supprime parfois avant classification. Mais on voit aussi \"movie\" et \"film\" qui sont spécifiques à notre jeu de données.\n","\n","Pour des raisons que je donnerai plus loin, on va utiliser un réseau convolutif (CNN) ici, et non un réseau récurrent (RNN). Il faut donc que tous les textes fassent la même taille. C'est la deuxième étape de la transformation. On va donc ajouter un certain nombre de 0 à toutes les listes qui ne font pas la taille désirée. C'est pour ça que le vocabulaire commence à 1 : cela laisse de la place au 0.\n","\n","**Exercice 5**\n","\n","Utiliser `keras.preprocessing.sequence` pour s'assurer que toutes les séquences soient longues de `MAX_LEN`, pas plus grandes et pas plus petites. (Ici, j'utilise `Xp_...` pour pad.)\n","\n","Mais comment définir MAX_LEN ? C'est ce qu'on appelle un hyperparamètre, par opposition aux paramètres directement appris par le classifieur. Il faut les définir avant d'entraîner les classifieurs.\n","\n","On peut essayer différentes valeurs pour voir ce qui marche le mieux, mais globalement, plus on prend en compte de mots dans la phrase, mieux c'est, même si ce sera plus lent. Ici, on a vu que la plupart des textes font moins de 1000 mots, mais on va utiliser 512 pour mieux se comparer à BERT qui est limité à 512 mots."]},{"cell_type":"code","metadata":{"id":"sinwb9maHqJ6","executionInfo":{"status":"ok","timestamp":1615379960245,"user_tz":-60,"elapsed":1564,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}}},"source":["from tensorflow.keras.preprocessing import sequence\n","\n","MAX_LEN = 512\n","\n","# À ton tour !\n","Xp_train = tf.keras.preprocessing.sequence.pad_sequences(Xi_train, MAX_LEN)\n","Xp_val = tf.keras.preprocessing.sequence.pad_sequences(Xi_val, MAX_LEN)\n","Xp_test = tf.keras.preprocessing.sequence.pad_sequences(Xi_test, MAX_LEN)\n","\n","assert Xp_train.shape == (20_000, MAX_LEN)\n","assert Xp_val.shape == (5_000, MAX_LEN)\n","assert Xp_test.shape == (25_000, MAX_LEN)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFlT1yw25LBX"},"source":["Vérifions que notre invariant sur les identifiants tient toujours :"]},{"cell_type":"code","metadata":{"id":"GOCCYtbpp9qL","executionInfo":{"status":"ok","timestamp":1615379963920,"user_tz":-60,"elapsed":1675,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}}},"source":["assert max([max(s) for s in Xp_train]) == VOCABULARY_SIZE - 1"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N170oTP1H_Jr"},"source":["Vérifions le fonctionnement de nos transformations en affichant le nombre de mots dans X_train et Xp_train."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"feEVHKXlHLfZ","executionInfo":{"status":"ok","timestamp":1615379965472,"user_tz":-60,"elapsed":745,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"9518f7de-e9c0-404e-938b-e1db3ceb8328"},"source":["import random\n","\n","for _ in range(5):\n","    i = random.randrange(len(X_train))\n","    print(\"positive\" if y_train[i] else \"negative\")\n","    print(X_train[i])\n","    print(len(X_train[i].split(\" \")), np.count_nonzero(Xp_train[i]))\n","    print()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["positive\n","This is one the few movies I can watch over and over. If you've never seen it, give it a shot. Richard Dreyfus and Raoul Julia are wonderful together and although the movie amuses me greatly, it reminds me of Julia's untimely demise. It is a good opportunity to sit back and laugh at the international intrigue that is too much with us in these time of terror and fear.\n","70 64\n","\n","positive\n","This is a very good movie. Do you want to know the real reasons why so many here are knocking this movie? I will tell you. In this movie, you have a black criminal who outwits a white professor. A black cop who tells the white professor he is wrong for defending the black criminal and the black cop turns out to be right, thus. making the white professor look stupid. It always comes down to race. This is an excellent movie. Pay no attention to the racist. If you can get over that there are characters who are played by blacks in this movie who outsmart the white characters, then you shouldn't have any problems enjoying this movie. I recommended everyone to go see this movie.\n","127 121\n","\n","negative\n","It seems that the people behind Envy realised that recent comedies - especially ones involving Ben Stiller and to a lesser degree Jack Black - have been situation spoofs, which have steadily declined in originality and generally laughs. I found the sheer absurdity of Zoolander utterly hilarious when it was released, Starsky and Hutch was also enjoyable, and then Dodgeball kept the laughs going for a lot of people, although personally i was a bit tired of the over-the-top characters - especially when the scenario wasn't quite so funny (perhaps the comedy of a Dodgeball tournament doesn't quite translate to Australia, where it's rarely played). So in an attempt to do something a little more original, Envy moves away from an absurd scenario and instead revolves around the absurd creation of Jack Black's character (i won't spoil what it is for those who intend to see the movie). The problem is that the movie seems to drag, i'm not a big enough movie buff to be able to think of examples, but it seems like this set up has been done a thousand times before - and very rarely successfully. So instead of a nice, crisp, enjoyable and fresh comedy, you get a film that seems to just go through the motions. Sure the motions can be quite amusing, and they're centred on an idea that is quirky enough to provide a few laughs - especially with Jack Black playing the excited and amusing, though a bit 2D, creator. Ben Stiller on the other hand seems a bit lost, he's asked to play a fuller role than the ridiculous characters of his Zoolander breed of movies, but he struggles as a family man, whether his fault or the scripts, there isn't enough depth to the character and the result is a movie of Ben Stiller doing those typical mannerisms and generally becoming tedious. The performance doesn't leave an imprint on the viewer (he's just Ben Stiller, Jack Black manages to actually portray a character - though not a challenging one). The last annoying element of the movie is Christopher Walken's role as 'The J Man', which is about as typical and two dimensional as characters come, and naturally he becomes monotonous and frustrating very quickly.<br /><br />It's really not as unbearable as some people would have you think, it's watchable, especially if you're in the right mood (feeling silly would be a good prerequisite for seeing this film). Hire it on a movie night with friends and watch it after you've watched a scary film and feel like something light - hopefully you'll also be somewhat tipsy by then too. In that scenario i can imagine it would be quite enjoyable, but generally it provides too few laughs to carry itself and most of the time just drags along.\n","469 433\n","\n","positive\n","This isn't exactly a great film, but I admire the writers and director for trying something a little different. The film's main theme is fate and small, seemingly insignificant things that can greatly change the future. In some ways this reminds me of the film SLIDING DOORS, though instead of focusing on one random event, seemingly random stuff happens repeatedly and each one helps build to the cute conclusion. Plus, an odd bald guy seems to understand all this and he talks about this during one brief scene--like he's some sort of omnipotent being but there's absolutely no explanation of him in the film (like the two guys that fight each other in the clock tower in THE HUDSUCKER PROXY).<br /><br />The DVD jacket shows just Audrey Tautou. This is capitalize on her success in AMELIE, though she is only one of many actors in the film and there is no one starring role. The pace is brisk, the acting fine and the conclusion isn't bad at all. The only reason I didn't score it higher is that some of the characters were a bit uninteresting and I think the movie could have perhaps been tightened up with a few less subplots.\n","202 189\n","\n","negative\n","Yes, people are racist. People are even racist in college. That's a good point, and the issue of racism has been dealt with many times before in countless films. What sets Higher Learning apart from the pack is that it deals with the issue of racism in the most ham-fisted and predictable way possible, oh yeah it's in college too.<br /><br />This film deals with this problem of racism the way Frankenstein deals with most problems, it bashes you over the head repeatedly in a brutal and sluggish manner. Most of the characters are cartoonish, one-dimensional, caricatures (lesbian feminist, angry black man), that react to situations as dramatically and predictably as possible. Instead of defying stereotypes this film is overpopulated with them. The angry black men feel cheated, feminists hate men, etc. (one feminist even holds a sign that reads \"Dead Men Don't Rape.\" See what I mean?) I don't want to give anything away, but in this movie if someone seems like a shifty loner or a date rapist they'll probably behave exactly how you expect them to. The changes the characters go through seems obvious to everyone but the people in the movie. The big twist in the plot hinges on whether or not the violent neo-Nazis will act like violent neo-Nazis. I'll guess you'll just have to watch to find out what happens.<br /><br />Another problem I have with this movie is that it's supposed to be \"gritty\" and \"hard-hitting,\" but they make Nazis the bad guys. I agree Nazis are evil, but that's my point. Everybody thinks Nazis are bad; we're not breaking any new ground here. Nazis have been portrayed as villains since the 1930's. The film doesn't challenge any viewpoints or make bold statements. It just deals with issues we all know about in a clumsy, after-school-special like, manner. Being anti-rape, anti-racist, and anti-Nazi isn't exactly taking a hard stance on a controversial issue.<br /><br />Higher Learning is predictable, cartoonish, and in a word stupid. Avoid at all costs.\n","335 325\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FsQ4-8vyIF8A"},"source":["Il y a en général moins de mots dans `Xp_train` que dans `X_train`, parce que le tokenizer Keras filtre des caractères peu fréquents."]},{"cell_type":"markdown","metadata":{"id":"8MujZ9Y4HqKX"},"source":["### Modèle Keras : réseau convolutif\n","\n","(Je me suis inspiré https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py.)\n","\n","On va entraîner un réseau de neurones pour apprendre à classifier nos textes. [Des modèles avancés d'analyse d'opinion utilisent une analyse syntaxique](http://nlp.stanford.edu:8080/sentiment/rntnDemo.html) pour détecter par exemple que \"pas mal\" est positif, alors que \"mal\" est négatif. Ici, le mieux qu'on puisse espérer, c'est que notre modèle puisse reconnaître certains mots ou séquences de mots comme positifs ou négatifs.\n","\n","Pour cette raison, on va commencer par utiliser un réseau convolutif (CNN) au lieu d'un réseau récurrent (RNN). En effet, selon  [Yin et al., 2017](https://arxiv.org/pdf/1702.01923.pdf), le seul cas où on ne veut pas utiliser un réseau récurrent est typiquement quand on veut simplement reconnaître des mots, comme pour l'analyse d'opinion.\n","\n","On utilise l'API Sequential de Keras :"]},{"cell_type":"code","metadata":{"id":"WZr7ZJCaHqKY","executionInfo":{"status":"ok","timestamp":1615380035048,"user_tz":-60,"elapsed":5965,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}}},"source":["from tensorflow.keras.models import Sequential\n","\n","cnn_model = Sequential()"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MPThT-YgHqKa"},"source":["Ensuite, on utilise une couche d'embeddings, comme expliqué plus haut. La [couche de Dropout](https://en.wikipedia.org/wiki/Dropout_(neural_networks)) est simplement là pour aléatoirement supprimer des neurones de la couche d'Embeddings, ce qui oblige le réseau à généraliser en ne dépendant pas de la valeur particulière d'un neurone.\n","\n","On utilise 50 dimensions pour les embeddings, parce qu'on a peu de données : il y a donc peu de chances ici d'avoir des meilleurs résultats en essayant une dimension supérieure (mais vous pouvez essayer !)"]},{"cell_type":"code","metadata":{"id":"xbzWoy9BHqKc","executionInfo":{"status":"ok","timestamp":1615380036690,"user_tz":-60,"elapsed":840,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}}},"source":["from tensorflow.keras.layers import Embedding, Dropout\n","\n","EMBEDDING_DIMS = 50\n","\n","# we start off with an efficient embedding layer which maps\n","# our vocab indices into embedding_dims dimensions\n","cnn_model.add(\n","    Embedding(\n","        input_dim=VOCABULARY_SIZE,\n","        output_dim=EMBEDDING_DIMS,\n","        input_length=MAX_LEN,\n","        mask_zero=True\n","    )\n",")\n","cnn_model.add(Dropout(0.2))"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eJwZC1h-HqKf"},"source":["Ensuite, on ajoute la couche de convolution, puis une couche plus classique complètement connectée.\n","\n","La dernière couche est la couche de sortie, qui ne contient qu'un seul neurone, qui dira si le résultat est posifif non."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TOuEROSIHqKg","executionInfo":{"status":"ok","timestamp":1615380044080,"user_tz":-60,"elapsed":886,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"65e63471-6cf8-461a-d17f-71a6eaad56be"},"source":["from tensorflow.keras.layers import Dense, Dropout, Activation\n","from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n","\n","FILTERS = 250\n","KERNEL_SIZE = 10\n","\n","# we add a Convolution1D, which will learn\n","# word group filters of size FILTERS:\n","cnn_model.add(\n","    Conv1D(\n","        filters=FILTERS,\n","        kernel_size=KERNEL_SIZE,\n","        padding=\"valid\",\n","        activation=\"relu\",\n","        strides=1,\n","    )\n",")\n","# we use max pooling:\n","cnn_model.add(GlobalMaxPooling1D())\n","\n","# We add a vanilla hidden layer:\n","cnn_model.add(Dense(250))\n","cnn_model.add(Dropout(0.2))\n","cnn_model.add(Activation(\"relu\"))\n","\n","# We project onto a single unit output layer, and squash it with a cnn_tokenizer = tsigmoid:\n","cnn_model.add(Dense(1))\n","cnn_model.add(Activation(\"sigmoid\"))\n","\n","print(cnn_model.summary())"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 512, 50)           250000    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 512, 50)           0         \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, 503, 250)          125250    \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 250)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 250)               62750     \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 250)               0         \n","_________________________________________________________________\n","activation (Activation)      (None, 250)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 251       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 1)                 0         \n","=================================================================\n","Total params: 438,251\n","Trainable params: 438,251\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EXPOAZsmHqKj"},"source":["\n","\n","**Exercice 6**\n","\n","On peut désormais compiler le modèle et l'entraîner.\n","\n","Compilation :\n","\n"," * Comme il s'agit de classification binaire, on utilise `binary_crossentropy`.\n"," * L'optimizer `adam` est un bon choix par défaut, et il est rapide.\n"," * On utilise la métrique `accuracy`, les classes étant équilibrées, pas besoin de se préoccuper de précision ou rappel.\n","\n","Entrâinement :\n"," * Taille de batch de 128 (le modèle est petit, donc on peut avoir des tailles un peu plus grandes)\n"," * 2 époques\n"," * Passer les données d'entraînement et de validation via `cnn_train_dataset.shuffle(1_000).batch(128)` et `cnn_val_dataset.batch(128)`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":422},"id":"3DKug6-7HqKk","executionInfo":{"status":"ok","timestamp":1615382571470,"user_tz":-60,"elapsed":11154,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"31a9f3ca-0554-4525-8afe-f02c5b834108"},"source":["# Prepare TensorFlow datasets for faster GPU ingestion\n","cnn_train_dataset = tf.data.Dataset.from_tensor_slices(\n","    (Xp_train, np.array(y_train)))\n","cnn_val_dataset = tf.data.Dataset.from_tensor_slices(\n","    (Xp_val, np.array(y_val)))\n","cnn_test_dataset = tf.data.Dataset.from_tensor_slices(\n","    (Xp_test, np.array(y_test)))\n","\n","# À toi de jouer !\n","cnn_model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","history = cnn_model.fit(cnn_train_dataset.shuffle(1_000).batch(128), validation_data=cnn_val_dataset.batch(128), epochs = 2, batch_size=128)\n","\n","plot_history(history)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n","157/157 [==============================] - 5s 31ms/step - loss: 0.0106 - accuracy: 0.9962 - val_loss: 2.6280 - val_accuracy: 0.5718\n","Epoch 2/2\n","157/157 [==============================] - 5s 30ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 2.7358 - val_accuracy: 0.5712\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAmEAAAFNCAYAAABIc7ibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5hddX3v8fc31wkhBEmiIgGSyi3QGIIjVrECipWLkuM98XgkQkGpN6yXwjk+QlFOW8FqVWoLBVSKRKTWExWKNYrYqm0ChACJaIhcAqghEm4xJJN8zx97T9iZ7D2zk8ya32Tm/XqePFnrt35rre/av9k7n6y19prITCRJkjSwRpQuQJIkaTgyhEmSJBVgCJMkSSrAECZJklSAIUySJKkAQ5gkSVIBhjBJg15ETIuIjIhRbfSdHxH/MRB1SdKuMIRJ6lcRcV9EbIyIyT3ab68HqWllKpOkwcUQJqkKvwLmdc9ExExgj3LlDA7tnMmTNHwYwiRV4WrgnQ3zpwFfbewQERMj4qsRsSYi7o+Ij0fEiPqykRFxSUQ8GhGrgFOarHtFRDwSEQ9FxKciYmQ7hUXENyLi1xHxeETcEhFHNCwbFxGfqdfzeET8R0SMqy97RUT8JCLWRcSDETG/3n5zRPxpwza2uRxaP/v33oj4JfDLetvf1bfxRETcGhF/3NB/ZET874i4NyKerC/fPyIujYjP9DiWhRHxoXaOW9LgYwiTVIWfAXtFxIx6OJoL/HOPPl8AJgJ/ABxLLbS9q77sTOB1wGygE3hzj3W/DHQBB9X7/Anwp7TnRuBg4LnAbcA1DcsuAV4MvBzYB/gYsCUiDqyv9wVgCnAksLTN/QH8D+ClwOH1+cX1bewDfA34RkR01Jf9ObWziCcDewGnA+uBrwDzGoLqZOCE+vqSdkOGMElV6T4b9hpgBfBQ94KGYHZeZj6ZmfcBnwH+V73LW4HPZeaDmfk74K8a1n0etYByTmY+nZm/BT5b316fMvPK+j6fAS4AZtXPrI2gFng+mJkPZebmzPxJvd/bge9n5rWZuSkz12bmjoSwv8rM32Xm7+s1/HN9G12Z+RlgLHBove+fAh/PzHuy5o563/8GHgdeXe83F7g5M3+zA3VIGkS8P0FSVa4GbgGm0+NSJDAZGA3c39B2P7BfffoFwIM9lnU7sL7uIxHR3TaiR/+m6uHvIuAt1M5obWmoZyzQAdzbZNX9W7S3a5vaIuIjwBnUjjOpnfHq/iJDb/v6CvAO4N/rf//dLtQkqTDPhEmqRGbeT+0G/ZOBb/ZY/CiwiVqg6nYAz54te4RaGGlc1u1B4BlgcmbuXf+zV2YeQd/eDsyhdhlvIjCt3h71mjYAL2yy3oMt2gGeZtsvHTy/SZ/snqjf//Uxamf7npOZe1M7w9WdKHvb1z8DcyJiFjAD+FaLfpJ2A4YwSVU6A3hVZj7d2JiZm4HrgIsiYkL9nqs/59n7xq4DPhARUyPiOcC5Des+AnwP+ExE7BURIyLihRFxbBv1TKAW4NZSC07/t2G7W4Argb+NiBfUb5B/WUSMpXbf2AkR8daIGBURkyLiyPqqS4E3RsQeEXFQ/Zj7qqELWAOMiohPUDsT1u2fgE9GxMFR86KImFSvcTW1+8muBv6l+/KmpN2TIUxSZTLz3sxc0mLx+6mdRVoF/Ae1G8yvrC+7HLgJuIPazfM9z6S9ExgDLAceA64H9m2jpK9Su7T5UH3dn/VY/hHgTmpB53fA3wAjMvMBamf0PlxvXwrMqq/zWWAj8BtqlwuvoXc3Af8G/KJeywa2vVz5t9RC6PeAJ4ArgHENy78CzKQWxCTtxiIz++4lSRoUIuKV1M4YHph+gEu7Nc+ESdJuIiJGAx8E/skAJu3+DGGStBuIiBnAOmqXXT9XuBxJ/cDLkZIkSQV4JkySJKkAQ5gkSVIBu90T8ydPnpzTpk0rXYYkSVKfbr311kczc0qzZbtdCJs2bRpLlrR67JAkSdLgERH3t1rm5UhJkqQCDGGSJEkFGMIkSZIK2O3uCWtm06ZNrF69mg0bNpQuZcjo6Ohg6tSpjB49unQpkiQNSUMihK1evZoJEyYwbdo0IqJ0Obu9zGTt2rWsXr2a6dOnly5HkqQhaUhcjtywYQOTJk0ygPWTiGDSpEmeWZQkqUKVhbCIuDIifhsRd7VYHhHx+YhYGRHLIuKoXdzfrqyuHnw9JUmqVpVnwr4MnNjL8pOAg+t/zgK+VGEtlVq7di1HHnkkRx55JM9//vPZb7/9ts5v3Lix13WXLFnCBz7wgQGqVJIkDRaV3ROWmbdExLReuswBvpq13yD+s4jYOyL2zcxHqqqpKpMmTWLp0qUAXHDBBey555585CMf2bq8q6uLUaOav9SdnZ10dnYOSJ2SJGnwKHlj/n7Agw3zq+tt24WwiDiL2tkyDjjggAEpblfNnz+fjo4Obr/9do455hjmzp3LBz/4QTZs2MC4ceO46qqrOPTQQ7n55pu55JJL+M53vsMFF1zAAw88wKpVq3jggQc455xzPEsmacir/V8cMiEb2p6drv9Nbp1u1Z6N22vs10bfxu11NzZuo7t9uzobauqz7za1Z8P09jVsne6xvR05/mf7N6unl+PPxrXaOyaa1Nnu8fc21o3H1LJPq/Fvub3a9Kz9J3LECyZSym7x7cjMvAy4DKCzszP76L5LtmxJurZsqe13myKa1NVk/a7NW9i0eQubtyQPPPggi370Y0aOHMkTjz/B9xbdzKhRo/jBou/zF+eex7Vfv45nNm1m85Zk/cYuNm3ewvIVK7jxe9/nySefZPbMIzjt9DMZPXp00331Wk82be1zO42rbdi0mR/8/Df1N+qzy3foDd7iTUtDn9raPT9wnu3b6xu81TYaOm3Tp+kHThs1tTimZ9dp/gZvdvz0+CBq/ho1rN/ste/ldW6sqa/XqOn49dG3+Qd0z2Pavr31B3Sr42/9s7PD49fyZ6TVsTZ8iPeoodfxa/K60Kpvi9eoWQ29HVPr16X1WDerc5tjavqPWKvXpdnPeOtj3b4eqZxzTzps2Iawh4D9G+an1tt2yV9++26WP/zETq+/eUuyYdPmbdqmTxnPmX/8B22tv/bpjfw+n+GJ32/i5Se8jlWPrgfg1w8/xF9/4lwe+NW9RARdXV388rdPsXrd73l6Yxcrf/sUv3t6Iy/541fz4OObgA4m7jOZ/16xiuftu99OH8+uePSpjZy50N/TWYUIiK3T0TANW+ca/oqt09Ew/ewXKKJH/63bbdH32e9dbNunZw0969ym/oaaejsmejnWnsfUuJ9tjrtHTc8ec5O+rbbR8AJF97IR3e2x3TH1/G7K9uPU6pi2r7Pp+MX2NbU8/h7tNNveNrU1jEmLse5r/J497lZ9tj9+eunb/vi1qLNHTb323dHxa1FTX8ff9vg1Of5mNfV2TM3fqz2OqXFMW/3s9HJMrcev1fg379vbMbU81hY/I61/fpv/7LDNOt2vS+ufs/Fjy56LKrn3hcD7ImIB8FLg8cFwP9iICMaOHrlN24Sxo9n/OXs0/KTW9JgFYOK40YzfYzTjx45i/+c+hwP32QOAvz7307z2Na/iPX/2Te6//35e99rXcOCk8TywVwd7jB7FtEnj2XvcaMbvuSfTJo0HoGPMKJ4/YQwHTh7fcn+91bLNu7DXfs23lY+N5f+995haW8sPkSZvnF4/tHt5g7P9G4Rt1m3vQ6vVP1C9fRCxzQfYDr7B2/0g6vmvgiRpWKsshEXEtcBxwOSIWA2cD4wGyMx/AG4ATgZWAuuBd/XHfs9//RH9sZmd1jF6JOPGjGLMqBGMHzuKiXuMAeD3Tz/JQdMOZOK4MXxzwTWMiFpg23PsKEaNDPYaN5qxo0fSMXoke42rPaV+RAQTOkazV0eZp9aPGTWCGfvvXWTfkiQNdVV+O3JeH8sTeG9V+x9sPvaxj3HaaafxqU99ilNOOaV0OZIkqbDI3ezuyM7OzlyyZNv7lFasWMGMGTMKVTR0+bpKkrRrIuLWzGz6LKoh8WuLJEmSdjeGMEmSpAIMYZIkSQUYwiRJkgowhEmSJBVgCJMkSSrAENYPjj/+eG666aZt2j73uc9x9tlnN+1/3HHH0f2YjZNPPpl169Zt1+eCCy7gkksu6XW/3/rWt1i+fPnW+U984hN8//vf39HyJUlSAYawfjBv3jwWLFiwTduCBQuYN6/X59UCcMMNN7D33jv3VPqeIezCCy/khBNO2KltSZKkgWUI6wdvfvOb+e53v8vGjRsBuO+++3j44Ye59tpr6ezs5IgjjuD8889vuu60adN49NFHAbjooos45JBDeMUrXsE999yztc/ll1/OS17yEmbNmsWb3vQm1q9fz09+8hMWLlzIRz/6UY488kjuvfde5s+fz/XXXw/AokWLmD17NjNnzuT000/nmWee2bq/888/n6OOOoqZM2fy85//vMqXRpIktWAI6wf77LMPRx99NDfeeCNQOwv21re+lYsuuoglS5awbNkyfvSjH7Fs2bKW27j11ltZsGABS5cu5YYbbmDx4sVbl73xjW9k8eLF3HHHHcyYMYMrrriCl7/85Zx66qlcfPHFLF26lBe+8IVb+2/YsIH58+fz9a9/nTvvvJOuri6+9KUvbV0+efJkbrvtNs4+++w+L3lKkqRqVPa7I4u58Vz49Z39u83nz4ST/rrXLt2XJOfMmcOCBQu44ooruO6667jsssvo6urikUceYfny5bzoRS9quv6Pf/xj3vCGN7DHHnsAcOqpp25ddtddd/Hxj3+cdevW8dRTT/Ha176211ruuecepk+fziGHHALAaaedxqWXXso555wD1EIdwItf/GK++c1vtvcaSJKkfuWZsH4yZ84cFi1axG233cb69evZZ599uOSSS1i0aBHLli3jlFNOYcOGDTu17fnz5/PFL36RO++8k/PPP3+nt9Nt7NixAIwcOZKurq5d2pYkSdo5Q+9MWB9nrKqy5557cvzxx3P66aczb948nnjiCcaPH8/EiRP5zW9+w4033shxxx3Xcv1XvvKVzJ8/n/POO4+uri6+/e1v8+53vxuAJ598kn333ZdNmzZxzTXXsN9++wEwYcIEnnzyye22deihh3LfffexcuVKDjroIK6++mqOPfbYSo5bkiTtHM+E9aN58+Zxxx13MG/ePGbNmsXs2bM57LDDePvb384xxxzT67pHHXUUb3vb25g1axYnnXQSL3nJS7Yu++QnP8lLX/pSjjnmGA477LCt7XPnzuXiiy9m9uzZ3HvvvVvbOzo6uOqqq3jLW97CzJkzGTFiBO95z3v6/4AlSdJOi8wsXcMO6ezszO5nbHVbsWIFM2bMKFTR0OXrKknSromIWzOzs9kyz4RJkiQVYAiTJEkqwBAmSZJUwJAJYbvbvW2Dna+nJEnVGhIhrKOjg7Vr1xoc+klmsnbtWjo6OkqXIknSkDUknhM2depUVq9ezZo1a0qXMmR0dHQwderU0mVIkjRkDYkQNnr0aKZPn166DEmSpLYNicuRkiRJuxtDmCRJUgGGMEmSpAIMYZIkSQUYwiRJkgowhEmSJBVgCJMkSSrAECZJklSAIUySJKkAQ5gkSVIBhjBJkqQCDGGSJEkFGMIkSZIKMIRJkiQVYAiTJEkqwBAmSZJUgCFMkiSpAEOYJElSAYYwSZKkAgxhkiRJBRjCJEmSCjCESZIkFWAIkyRJKsAQJkmSVIAhTJIkqYBKQ1hEnBgR90TEyog4t8nyAyNiUUQsi4ibI2JqlfVIkiQNFpWFsIgYCVwKnAQcDsyLiMN7dLsE+Gpmvgi4EPirquqRJEkaTKo8E3Y0sDIzV2XmRmABMKdHn8OBH9Snf9hkuSRJ0pBUZQjbD3iwYX51va3RHcAb69NvACZExKQKa5IkSRoUSt+Y/xHg2Ii4HTgWeAjY3LNTRJwVEUsiYsmaNWsGukZJkqR+V2UIewjYv2F+ar1tq8x8ODPfmJmzgf9Tb1vXc0OZeVlmdmZm55QpUyosWZIkaWBUGcIWAwdHxPSIGAPMBRY2doiIyRHRXcN5wJUV1iNJkjRoVBbCMrMLeB9wE7ACuC4z746ICyPi1Hq344B7IuIXwPOAi6qqR5IkaTCJzCxdww7p7OzMJUuWlC5DkiSpTxFxa2Z2NltW+sZ8SZKkYckQJkmSVIAhTJIkqQBDmCRJUgGGMEmSpAIMYZIkSQUYwiRJkgowhEmSJBVgCJMkSSrAECZJklSAIUySJKkAQ5gkSVIBhjBJkqQCDGGSJEkFGMIkSZIKMIRJkiQVYAiTJEkqwBAmSZJUgCFMkiSpAEOYJElSAYYwSZKkAgxhkiRJBRjCJEmSCjCESZIkFWAIkyRJKsAQJkmSVIAhTJIkqQBDmCRJUgGGMEmSpAIMYZIkSQUYwiRJkgowhEmSJBVgCJMkSSrAECZJklSAIUySJKkAQ5gkSVIBhjBJkqQCDGGSJEkFGMIkSZIKMIRJkiQVYAiTJEkqoM8QFhGvjwjDmiRJUj9qJ1y9DfhlRHw6Ig6ruiBJkqThoM8QlpnvAGYD9wJfjoifRsRZETGh8uokSZKGqLYuM2bmE8D1wAJgX+ANwG0R8f4Ka5MkSRqy2rkn7NSI+FfgZmA0cHRmngTMAj5cbXmSJElD06g2+rwJ+Gxm3tLYmJnrI+KMasqSJEka2toJYRcAj3TPRMQ44HmZeV9mLqqqMEmSpKGsnXvCvgFsaZjfXG+TJEnSTmonhI3KzI3dM/XpMe1sPCJOjIh7ImJlRJzbZPkBEfHDiLg9IpZFxMntly5JkrT7aieErYmIU7tnImIO8GhfK0XESOBS4CTgcGBeRBzeo9vHgesyczYwF/j7dguXJEnanbVzT9h7gGsi4otAAA8C72xjvaOBlZm5CiAiFgBzgOUNfRLYqz49EXi4zbolSZJ2a32GsMy8F/ijiNizPv9Um9vej1pg67YaeGmPPhcA36s/b2w8cEKb25YkSdqttXMmjIg4BTgC6IgIADLzwn7Y/zzgy5n5mYh4GXB1RPxhZjZ+EYCIOAs4C+CAAw7oh91KkiSV1c7DWv+B2u+PfD+1y5FvAQ5sY9sPAfs3zE+ttzU6A7gOIDN/CnQAk3tuKDMvy8zOzOycMmVKG7uWJEka3Nq5Mf/lmflO4LHM/EvgZcAhbay3GDg4IqZHxBhqN94v7NHnAeDVABExg1oIW9Nu8ZIkSburdkLYhvrf6yPiBcAmar8/sleZ2QW8D7gJWEHtW5B3R8SFDd+2/DBwZkTcAVwLzM/M3NGDkCRJ2t20c0/YtyNib+Bi4DZq32i8vJ2NZ+YNwA092j7RML0cOKbtaiVJkoaIXkNYRIwAFmXmOuBfIuI7QEdmPj4g1UmSJA1RvV6OrH9L8dKG+WcMYJIkSbuunXvCFkXEm6L72RSSJEnaZe2EsHdT+4Xdz0TEExHxZEQ8UXFdkiRJQ1o7T8yfMBCFSJIkDSd9hrCIeGWz9sy8pf/LkSRJGh7aeUTFRxumO6j9Yu5bgVdVUpEkSdIw0M7lyNc3zkfE/sDnKqtIkiRpGGjnxvyeVgMz+rsQSZKk4aSde8K+QO0p+VALbUdSe3K+JEmSdlI794QtaZjuAq7NzP+sqB5JkqRhoZ0Qdj2wITM3A0TEyIjYIzPXV1uaJEnS0NXWE/OBcQ3z44DvV1OOJEnS8NBOCOvIzKe6Z+rTe1RXkiRJ0tDXTgh7OiKO6p6JiBcDv6+uJEmSpKGvnXvCzgG+EREPAwE8H3hbpVVJkiQNce08rHVxRBwGHFpvuiczN1VbliRJ0tDW5+XIiHgvMD4z78rMu4A9I+LPqi9NkiRp6GrnnrAzM3Nd90xmPgacWV1JkiRJQ187IWxkRET3TESMBMZUV5IkSdLQ186N+f8GfD0i/rE+/27gxupKkiRJGvraCWF/AZwFvKc+v4zaNyQlSZK0k/q8HJmZW4D/Au4DjgZeBayotixJkqShreWZsIg4BJhX//Mo8HWAzDx+YEqTJEkaunq7HPlz4MfA6zJzJUBEfGhAqpIkSRriersc+UbgEeCHEXF5RLya2hPzJUmStItahrDM/FZmzgUOA35I7dcXPTcivhQRfzJQBUqSJA1F7dyY/3Rmfi0zXw9MBW6n9o1JSZIk7aR2Hta6VWY+lpmXZearqypIkiRpONihECZJkqT+YQiTJEkqwBAmSZJUgCFMkiSpAEOYJElSAYYwSZKkAgxhkiRJBRjCJEmSCjCESZIkFWAIkyRJKsAQJkmSVIAhTJIkqQBDmCRJUgGGMEmSpAIMYZIkSQUYwiRJkgowhEmSJBVgCJMkSSrAECZJklSAIUySJKmASkNYRJwYEfdExMqIOLfJ8s9GxNL6n19ExLoq65EkSRosRlW14YgYCVwKvAZYDSyOiIWZuby7T2Z+qKH/+4HZVdUjSZI0mFR5JuxoYGVmrsrMjcACYE4v/ecB11ZYjyRJ0qBRZQjbD3iwYX51vW07EXEgMB34QYX1SJIkDRqD5cb8ucD1mbm52cKIOCsilkTEkjVr1gxwaZIkSf2vyhD2ELB/w/zUelszc+nlUmRmXpaZnZnZOWXKlH4sUZIkqYwqQ9hi4OCImB4RY6gFrYU9O0XEYcBzgJ9WWIskSdKgUlkIy8wu4H3ATcAK4LrMvDsiLoyIUxu6zgUWZGZWVYskSdJgU9kjKgAy8wbghh5tn+gxf0GVNUiSJA1Gg+XGfEmSpGHFECZJklSAIUySJKkAQ5gkSVIBhjBJkqQCDGGSJEkFGMIkSZIKMIRJkiQVYAiTJEkqwBAmSZJUgCFMkiSpAEOYJElSAYYwSZKkAgxhkiRJBRjCJEmSCjCESZIkFWAIkyRJKsAQJkmSVIAhTJIkqQBDmCRJUgGGMEmSpAIMYZIkSQUYwiRJkgowhEmSJBVgCJMkSSrAECZJklSAIUySJKkAQ5gkSVIBhjBJkqQCDGGSJEkFGMIkSZIKMIRJkiQVYAiTJEkqwBAmSZJUgCFMkiSpAEOYJElSAYYwSZKkAgxhkiRJBRjCJEmSCjCESZIkFWAIkyRJKsAQJkmSVIAhTJIkqQBDmCRJUgGGMEmSpAIMYZIkSQUYwiRJkgowhEmSJBVgCJMkSSqg0hAWESdGxD0RsTIizm3R560RsTwi7o6Ir1VZjyRJ0mAxqqoNR8RI4FLgNcBqYHFELMzM5Q19DgbOA47JzMci4rlV1SNJkjSYVHkm7GhgZWauysyNwAJgTo8+ZwKXZuZjAJn52wrrkSRJGjSqDGH7AQ82zK+utzU6BDgkIv4zIn4WESdWWI8kSdKgUdnlyB3Y/8HAccBU4JaImJmZ6xo7RcRZwFkABxxwwEDXKEmS1O+qPBP2ELB/w/zUeluj1cDCzNyUmb8CfkEtlG0jMy/LzM7M7JwyZUplBUuSJA2UKkPYYuDgiJgeEWOAucDCHn2+Re0sGBExmdrlyVUV1iRJkjQoVBbCMrMLeB9wE7ACuC4z746ICyPi1Hq3m4C1EbEc+CHw0cxcW1VNkiRJg0VkZukadkhnZ2cuWbKkdBmSJEl9iohbM7Oz2TKfmC9JklSAIUySJKkAQ5gkSVIBhjBJkqQCDGGSJEkFGMIkSZIKMIRJkiQVYAiTJEkqwBAmSZJUgCFMkiSpgFGlCxh0HroVfnppi4XRorlZ+470bdF/R/ru8LZbdB00dfdDHVVuu9/GZhfraNl/uLx+/bHtiupo2X+ovX79cYytdunrV37brXY5RMZmymEw6YUttlE9Q1hPG56AR+7Yvr3l79hs0r4jfVv2b9W3xab7ZdtV1j3Qr19/bbuiOlr2H2qv3w4coyQNtBP+El5xTrHdG8J6euHx8P5bS1chqVur4GeIHQTbrqqOFv2H3OtX5c/fLtaxo/0Hy+u3o9uesG+LbQ8MQ5ikwa3V5YYdvXwiSYOMN+ZLkiQVYAiTJEkqwBAmSZJUgCFMkiSpAEOYJElSAYYwSZKkAgxhkiRJBRjCJEmSCjCESZIkFWAIkyRJKiByR37v1CAQEWuA+yvezWTg0Yr3oR3nuAw+jsng5LgMPo7J4DQQ43JgZk5ptmC3C2EDISKWZGZn6Tq0Lcdl8HFMBifHZfBxTAan0uPi5UhJkqQCDGGSJEkFGMKau6x0AWrKcRl8HJPByXEZfByTwanouHhPmCRJUgGeCZMkSSpgWIewiDgxIu6JiJURcW6T5WMj4uv15f8VEdMGvsrhp41x+fOIWB4RyyJiUUQcWKLO4aSvMWno96aIyIjwW2AVa2dMIuKt9ffK3RHxtYGucThq4/PrgIj4YUTcXv8MO7lEncNJRFwZEb+NiLtaLI+I+Hx9zJZFxFEDVduwDWERMRK4FDgJOByYFxGH9+h2BvBYZh4EfBb4m4Gtcvhpc1xuBzoz80XA9cCnB7bK4aXNMSEiJgAfBP5rYCscftoZk4g4GDgPOCYzjwDOGfBCh5k23ysfB67LzNnAXODvB7bKYenLwIm9LD8JOLj+5yzgSwNQEzCMQxhwNLAyM1dl5kZgATCnR585wFfq09cDr46IGMAah6M+xyUzf5iZ6+uzPwOmDnCNw0077xWAT1L7j8qGgSxumGpnTM4ELs3MxwAy87cDXONw1M64JLBXfXoi8PAA1jcsZeYtwO966TIH+GrW/AzYOyL2HYjahnMI2w94sGF+db2taZ/M7AIeByYNSHXDVzvj0ugM4MZKK1KfY1I/fb9/Zn53IAsbxtp5nxwCHBIR/xkRP4uI3s4EqH+0My4XAO+IiNXADcD7B6Y09WJH/93pN6MGYidSFSLiHUAncGzpWoaziBgB/C0wv3Ap2tYoapdXjqN2tviWiJiZmeuKVqV5wJcz8zMR8TLg6oj4w8zcUrowDbzhfCbsIWD/hvmp9bamfSJiFLVTx2sHpLrhq51xISJOAP4PcGpmPjNAtQ1XfY3JBOAPgZsj4j7gj4CF3pxfqXbeJ6uBhZm5KTN/BfyCWihTddoZlzOA6wAy86dAB7XfX6hy2vp3pwrDOYQtBg6OiOkRMYbaDZILe/RZCJxWn34z8IP0wWpV63NcImI28I/UApj3uVSv1zHJzMczc3JmTsvMadTu0zs1M5eUKXdYaOfz61vUzoIREZOpXZlLuq0AAAJ+SURBVJ5cNZBFDkPtjMsDwKsBImIGtRC2ZkCrVE8LgXfWvyX5R8DjmfnIQOx42F6OzMyuiHgfcBMwErgyM++OiAuBJZm5ELiC2qnildRu6ptbruLhoc1xuRjYE/hG/XsSD2TmqcWKHuLaHBMNoDbH5CbgTyJiObAZ+Ghmeia/Qm2Oy4eByyPiQ9Ru0p/vf+6rFRHXUvsPyeT6vXjnA6MBMvMfqN2bdzKwElgPvGvAanPsJUmSBt5wvhwpSZJUjCFMkiSpAEOYJElSAYYwSZKkAgxhkiRJBRjCJA0pEbE5IpY2/Dm3H7c9LSLu6q/tSRrehu1zwiQNWb/PzCNLFyFJffFMmKRhISLui4hPR8SdEfHfEXFQvX1aRPwgIpZFxKKIOKDe/ryI+NeIuKP+5+X1TY2MiMsj4u6I+F5EjCt2UJJ2a4YwSUPNuB6XI9/WsOzxzJwJfBH4XL3tC8BXMvNFwDXA5+vtnwd+lJmzgKOAu+vtBwOXZuYRwDrgTRUfj6QhyifmSxpSIuKpzNyzSft9wKsyc1VEjAZ+nZmTIuJRYN/M3FRvfyQzJ0fEGmBq4y+Ij4hpwL9n5sH1+b8ARmfmp6o/MklDjWfCJA0n2WJ6RzzTML0Z762VtJMMYZKGk7c1/P3T+vRPgLn16f8J/Lg+vQg4GyAiRkbExIEqUtLw4P/gJA014yJiacP8v2Vm92MqnhMRy6idzZpXb3s/cFVEfBRYA7yr3v5B4LKIOIPaGa+zgUcqr17SsOE9YZKGhfo9YZ2Z+WjpWiQJvBwpSZJUhGfCJEmSCvBMmCRJUgGGMEmSpAIMYZIkSQUYwiRJkgowhEmSJBVgCJMkSSrg/wOgpv6Q7f/64gAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 720x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"i9G0OfPfHqKo"},"source":["La `val_accuracy` de 89% est prometteuse, mais on espère surtout que le modèle va mieux généraliser sur les données de test. En attendant, on peut regarder les paramètres appris de plus près.\n","\n","### Inspection de la couche d'embeddings\n","\n","Une des manières de voir si le réseau a bien appris, c'est de voir les poids appris pour la couche des embeddings. `model.layers` nous permet d'afficher toutes nos couches. Voyez plutôt :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"znZqsgz_HqKp","executionInfo":{"status":"ok","timestamp":1615382404113,"user_tz":-60,"elapsed":1639,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"5451cad5-822f-49e1-cc8a-8fc539e28442"},"source":["print(\"\\n\".join(map(str, cnn_model.layers)))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f57a9dd29d0>\n","<tensorflow.python.keras.layers.core.Dropout object at 0x7f57a9dd2890>\n","<tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f57a9d97750>\n","<tensorflow.python.keras.layers.pooling.GlobalMaxPooling1D object at 0x7f57a9dc1850>\n","<tensorflow.python.keras.layers.core.Dense object at 0x7f57a9da67d0>\n","<tensorflow.python.keras.layers.core.Dropout object at 0x7f57abfc29d0>\n","<tensorflow.python.keras.layers.core.Activation object at 0x7f57a9dc5f50>\n","<tensorflow.python.keras.layers.core.Dense object at 0x7f57ab36ef50>\n","<tensorflow.python.keras.layers.core.Activation object at 0x7f57a9da6ed0>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LYOAUA7_HqKt"},"source":["Mais c'est la couche des embeddings qui nous intéresse ici. Est-ce qu'elle a appris quelque chose d'intéressant ?\n","\n","**Exercice 7**\n","\n","Obtenir les poids de la couche des embeddings dans la variable `embeddings_weights`."]},{"cell_type":"code","metadata":{"id":"ANuR3YJmHqKu","executionInfo":{"status":"ok","timestamp":1615385177694,"user_tz":-60,"elapsed":598,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}}},"source":["# À ton tour !\n","embeddings_weights = cnn_model.layers[0].get_weights()[0]\n","\n","import numpy as np\n","\n","assert type(embeddings_weights) == np.ndarray\n","assert len(embeddings_weights) == VOCABULARY_SIZE\n","assert len(embeddings_weights[0]) == EMBEDDING_DIMS"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zGlUxdRkHqKx"},"source":["**Exercice 8**\n","\n","Afficher les vecteurs des cinq mots les plus fréquents de notre vocabulaire."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3fyCu4JHqKy","scrolled":true,"executionInfo":{"status":"ok","timestamp":1615388166113,"user_tz":-60,"elapsed":699,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"ff34277d-f7f2-4ebc-cac7-8d9050c1676e"},"source":["for i in range(1, 5+1):\n","  print(embeddings_weights[0])"],"execution_count":66,"outputs":[{"output_type":"stream","text":["[ 2.0869553e-02  2.9634513e-02 -5.7141129e-03 -1.3558146e-02\n"," -1.9098168e-02  6.0811482e-02 -1.4743620e-02  9.9710571e-03\n"," -2.6726693e-02  9.4254618e-04 -2.0170955e-02 -5.3127315e-02\n","  2.6576983e-02  1.2785130e-02 -8.2569662e-03  8.0183178e-02\n","  2.0171257e-02 -9.1235936e-03 -5.5067991e-03  8.7104766e-03\n","  1.9148815e-02 -2.1435494e-02 -1.0113115e-02 -1.9214662e-02\n"," -3.9017662e-02 -2.4446847e-02 -2.3001097e-02  1.6336033e-02\n"," -2.2268200e-03  2.6313853e-02  5.9849401e-03 -1.7501108e-02\n"," -2.6128598e-04 -2.1073233e-02 -2.6583929e-02 -1.4982059e-03\n"," -2.8102349e-02 -1.1070061e-02 -4.9273134e-03  8.2778707e-03\n","  2.9207231e-02  9.3934229e-03  3.3618640e-02  5.9491768e-02\n","  3.1730622e-02 -1.1590712e-02 -3.7402740e-05 -2.8848005e-03\n"," -3.4433673e-03 -8.9796223e-03]\n","[ 2.0869553e-02  2.9634513e-02 -5.7141129e-03 -1.3558146e-02\n"," -1.9098168e-02  6.0811482e-02 -1.4743620e-02  9.9710571e-03\n"," -2.6726693e-02  9.4254618e-04 -2.0170955e-02 -5.3127315e-02\n","  2.6576983e-02  1.2785130e-02 -8.2569662e-03  8.0183178e-02\n","  2.0171257e-02 -9.1235936e-03 -5.5067991e-03  8.7104766e-03\n","  1.9148815e-02 -2.1435494e-02 -1.0113115e-02 -1.9214662e-02\n"," -3.9017662e-02 -2.4446847e-02 -2.3001097e-02  1.6336033e-02\n"," -2.2268200e-03  2.6313853e-02  5.9849401e-03 -1.7501108e-02\n"," -2.6128598e-04 -2.1073233e-02 -2.6583929e-02 -1.4982059e-03\n"," -2.8102349e-02 -1.1070061e-02 -4.9273134e-03  8.2778707e-03\n","  2.9207231e-02  9.3934229e-03  3.3618640e-02  5.9491768e-02\n","  3.1730622e-02 -1.1590712e-02 -3.7402740e-05 -2.8848005e-03\n"," -3.4433673e-03 -8.9796223e-03]\n","[ 2.0869553e-02  2.9634513e-02 -5.7141129e-03 -1.3558146e-02\n"," -1.9098168e-02  6.0811482e-02 -1.4743620e-02  9.9710571e-03\n"," -2.6726693e-02  9.4254618e-04 -2.0170955e-02 -5.3127315e-02\n","  2.6576983e-02  1.2785130e-02 -8.2569662e-03  8.0183178e-02\n","  2.0171257e-02 -9.1235936e-03 -5.5067991e-03  8.7104766e-03\n","  1.9148815e-02 -2.1435494e-02 -1.0113115e-02 -1.9214662e-02\n"," -3.9017662e-02 -2.4446847e-02 -2.3001097e-02  1.6336033e-02\n"," -2.2268200e-03  2.6313853e-02  5.9849401e-03 -1.7501108e-02\n"," -2.6128598e-04 -2.1073233e-02 -2.6583929e-02 -1.4982059e-03\n"," -2.8102349e-02 -1.1070061e-02 -4.9273134e-03  8.2778707e-03\n","  2.9207231e-02  9.3934229e-03  3.3618640e-02  5.9491768e-02\n","  3.1730622e-02 -1.1590712e-02 -3.7402740e-05 -2.8848005e-03\n"," -3.4433673e-03 -8.9796223e-03]\n","[ 2.0869553e-02  2.9634513e-02 -5.7141129e-03 -1.3558146e-02\n"," -1.9098168e-02  6.0811482e-02 -1.4743620e-02  9.9710571e-03\n"," -2.6726693e-02  9.4254618e-04 -2.0170955e-02 -5.3127315e-02\n","  2.6576983e-02  1.2785130e-02 -8.2569662e-03  8.0183178e-02\n","  2.0171257e-02 -9.1235936e-03 -5.5067991e-03  8.7104766e-03\n","  1.9148815e-02 -2.1435494e-02 -1.0113115e-02 -1.9214662e-02\n"," -3.9017662e-02 -2.4446847e-02 -2.3001097e-02  1.6336033e-02\n"," -2.2268200e-03  2.6313853e-02  5.9849401e-03 -1.7501108e-02\n"," -2.6128598e-04 -2.1073233e-02 -2.6583929e-02 -1.4982059e-03\n"," -2.8102349e-02 -1.1070061e-02 -4.9273134e-03  8.2778707e-03\n","  2.9207231e-02  9.3934229e-03  3.3618640e-02  5.9491768e-02\n","  3.1730622e-02 -1.1590712e-02 -3.7402740e-05 -2.8848005e-03\n"," -3.4433673e-03 -8.9796223e-03]\n","[ 2.0869553e-02  2.9634513e-02 -5.7141129e-03 -1.3558146e-02\n"," -1.9098168e-02  6.0811482e-02 -1.4743620e-02  9.9710571e-03\n"," -2.6726693e-02  9.4254618e-04 -2.0170955e-02 -5.3127315e-02\n","  2.6576983e-02  1.2785130e-02 -8.2569662e-03  8.0183178e-02\n","  2.0171257e-02 -9.1235936e-03 -5.5067991e-03  8.7104766e-03\n","  1.9148815e-02 -2.1435494e-02 -1.0113115e-02 -1.9214662e-02\n"," -3.9017662e-02 -2.4446847e-02 -2.3001097e-02  1.6336033e-02\n"," -2.2268200e-03  2.6313853e-02  5.9849401e-03 -1.7501108e-02\n"," -2.6128598e-04 -2.1073233e-02 -2.6583929e-02 -1.4982059e-03\n"," -2.8102349e-02 -1.1070061e-02 -4.9273134e-03  8.2778707e-03\n","  2.9207231e-02  9.3934229e-03  3.3618640e-02  5.9491768e-02\n","  3.1730622e-02 -1.1590712e-02 -3.7402740e-05 -2.8848005e-03\n"," -3.4433673e-03 -8.9796223e-03]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"__FLbTZCHqK1"},"source":["OK, ça ne nous avance pas énormément comme ça, mais je trouve intéressant de voir de ses propres yeux un vrai vecteur.\n","\n","Voici une fonction `closest(word)` qui va afficher les 10 mots les plus proches de `word` selon la distance de similarité cosinus entre vecteurs de mots."]},{"cell_type":"code","metadata":{"id":"9WnD-GcOHqK2","executionInfo":{"status":"ok","timestamp":1615388172057,"user_tz":-60,"elapsed":1404,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}}},"source":["from scipy.spatial.distance import cosine as cosine_distance\n","\n","def closest(word, cnn_tokenizer, embeddings_weights):\n","    word_index = cnn_tokenizer.word_index[word]\n","    if word_index > len(embeddings_weights):\n","        return\n","    \n","    word_embedding = embeddings_weights[word_index]\n","    distances = []\n","    for i, embedding in enumerate(embeddings_weights):\n","        if i in cnn_tokenizer.index_word:\n","            distances.append((cosine_distance(word_embedding, embedding), cnn_tokenizer.index_word[i]))\n","    for d, w in sorted(distances)[:10]:\n","        print(f\"d={d:.5f} {w:}\")"],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bPEgvq8IHqK5"},"source":["Avec cette fonction, on peut regarder quels mots sont proches selon l'embedding appris, et se rendre compte que le réseau a appris des choses intéressantes.\n","\n","Par exemple, \"boring\" est négatif, et se trouve proche de mots négatifs (jamais les mêmes). Le réseau a appris ça tout seul à partir des textes données ! C'est fort non ?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dTUhdFMHqK6","executionInfo":{"status":"ok","timestamp":1615388175272,"user_tz":-60,"elapsed":582,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"a68a31f1-8505-4019-f47c-3f1108e9a93b"},"source":["closest(\"boring\", cnn_tokenizer, embeddings_weights)"],"execution_count":68,"outputs":[{"output_type":"stream","text":["d=0.00000 boring\n","d=0.29860 ham\n","d=0.30872 pathetic\n","d=0.31318 99\n","d=0.33957 fate\n","d=0.34215 animals\n","d=0.34870 subplot\n","d=0.35684 begin\n","d=0.35738 exposure\n","d=0.36033 misses\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0Mnvq98PHqK9"},"source":["\"warm\" semble plus positif, et en effet, dans mon cas je l'ai vu proche de mots comme \"notch\" (de \"top notch\"), \"higher\" ou \"awesome\"."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8aflj1ZJHqK_","executionInfo":{"status":"ok","timestamp":1615388177693,"user_tz":-60,"elapsed":577,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"4b1919a6-3341-4b3f-fbea-77de5d99cbd6"},"source":["closest(\"warm\", cnn_tokenizer, embeddings_weights)"],"execution_count":69,"outputs":[{"output_type":"stream","text":["d=0.00000 warm\n","d=0.43202 thoughtful\n","d=0.43340 bollywood\n","d=0.43865 forbidden\n","d=0.44939 survive\n","d=0.46201 cannon\n","d=0.46424 couples\n","d=0.46537 fame\n","d=0.46603 putting\n","d=0.47876 rise\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cNLwnf5eHqLF"},"source":["**Exercice 9**\n","\n","En utilisant `closest()` comme ci-dessus, essayez de trouver un mot pour lesquel vous pensez expliquer pourquoi il est proche d'autres mots."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0JyPP0TaPq-","executionInfo":{"status":"ok","timestamp":1615388180546,"user_tz":-60,"elapsed":585,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"23e101fd-3de5-475f-a3eb-d237266fbc3a"},"source":["closest(\"great\", cnn_tokenizer, embeddings_weights)"],"execution_count":70,"outputs":[{"output_type":"stream","text":["d=0.00000 great\n","d=0.45863 anything\n","d=0.45937 manages\n","d=0.47320 roberts\n","d=0.49395 graham\n","d=0.49832 behind\n","d=0.50683 lesbian\n","d=0.50773 depicted\n","d=0.50995 messages\n","d=0.51335 tight\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Eg7BeTLaS88","executionInfo":{"status":"ok","timestamp":1615388183430,"user_tz":-60,"elapsed":811,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"23f56a17-002f-4761-c57f-d52d4f3ecd72"},"source":["closest(\"painful\", cnn_tokenizer, embeddings_weights)"],"execution_count":71,"outputs":[{"output_type":"stream","text":["d=0.00000 painful\n","d=0.49279 contrived\n","d=0.51150 accepted\n","d=0.51315 faith\n","d=0.52096 entertained\n","d=0.53064 can't\n","d=0.54010 haunting\n","d=0.55218 attractive\n","d=0.55456 obvious\n","d=0.56454 take\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N0UIG2UVHqLm"},"source":["## Évaluation sur les données de test\n","\n","**Exercice 12**\n","\n","Calculer le score sur les données de test."]},{"cell_type":"code","metadata":{"id":"dcku5DNYHqLo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615388869173,"user_tz":-60,"elapsed":6858,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"7518e3cc-362b-4e87-82c1-88bdd4c0b580"},"source":["# À toi de jouer !\n","cnn_model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","history = cnn_model.fit(cnn_test_dataset.shuffle(1_000).batch(128))\n"],"execution_count":74,"outputs":[{"output_type":"stream","text":["196/196 [==============================] - 6s 29ms/step - loss: 1.6662 - accuracy: 0.6173\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jusUxp66HqLq"},"source":["On a réussi à améliorer le score de la baseline ! On a réduit le taux d'erreur de 40%, ce qui est une amélioration conséquente.\n"]},{"cell_type":"markdown","metadata":{"id":"qWYaHP6Ly3aW"},"source":["## Déploiement en production\n","\n","La suite, c'est de déployer votre modèle en production. Nous devons pour cela enregistrer le tokenizer et le modèle."]},{"cell_type":"code","metadata":{"id":"9N_JJI6zx0LU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615388796549,"user_tz":-60,"elapsed":3730,"user":{"displayName":"Thibault Cros","photoUrl":"","userId":"16505124720447267734"}},"outputId":"91e5882a-3638-4d60-dcab-3f04d36eac82"},"source":["export_path = \"imdb_reviews_cnn_model/1\"\n","print('export_path = {}\\n'.format(export_path))\n","!mkdir -p {export_path}\n","\n","with open(f\"imdb_reviews_cnn_model/tokenizer.json\", \"w\") as f:\n","    f.write(cnn_tokenizer.to_json())\n","\n","tf.keras.models.save_model(\n","    cnn_model,\n","    export_path,\n","    overwrite=True,\n","    include_optimizer=True,\n","    save_format=None,\n","    signatures=None,\n","    options=None\n",")\n","\n","print('\\nSaved model:')\n","!zip -r imdb_reviews_cnn_model.zip imdb_reviews_cnn_model"],"execution_count":73,"outputs":[{"output_type":"stream","text":["export_path = imdb_reviews_cnn_model/1\n","\n","INFO:tensorflow:Assets written to: imdb_reviews_cnn_model/1/assets\n","\n","Saved model:\n","  adding: imdb_reviews_cnn_model/ (stored 0%)\n","  adding: imdb_reviews_cnn_model/1/ (stored 0%)\n","  adding: imdb_reviews_cnn_model/1/variables/ (stored 0%)\n","  adding: imdb_reviews_cnn_model/1/variables/variables.data-00000-of-00001 (deflated 11%)\n","  adding: imdb_reviews_cnn_model/1/variables/variables.index (deflated 63%)\n","  adding: imdb_reviews_cnn_model/1/assets/ (stored 0%)\n","  adding: imdb_reviews_cnn_model/1/saved_model.pb (deflated 88%)\n","  adding: imdb_reviews_cnn_model/tokenizer.json (deflated 68%)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wDjBdE_oRV88"},"source":["Téléchargez le zip imdb_reviews_model.zip depuis l'onglet \"Fichiers\" à gauche pour le futur TP déploiement."]},{"cell_type":"markdown","metadata":{"id":"AMb95qrS3dOa"},"source":["### Autres améliorations\n","\n","**Exercice 13 (optionnel)**\n","\n","Pour faire mieux, on peut nettoyer le texte pour enlever les symboles spéciaux comme `<br >`, tokenizer via spaCy, supprimer les stop words, ou encore utiliser les lemmes plutôt que les mots bruts afin de réduire la taille du vocabulaire.\n","\n","On peut aussi mieux tirer des profils des mots mis utilisés à la manière de [Sun et al.](https://arxiv.org/abs/1905.05583) en sélectionnant les 128 premiers tokens puis les 382 derniers, la fin des textes étant plus susceptibles de contenir la conclusion.\n","\n","Enfin, est-ce qu'utiliser des word embeddings pré-entraînés (eg. ceux de GloVe) peut améliorer les résultatst ? J'ai essayé sans succès, mais j'ai peut-être abandonné trop tôt, sans affiner le learning rate, par exemple."]}]}